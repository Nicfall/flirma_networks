---
title: "Florida 16S analysis"
author: "Nicola G. Kriefall"
date: "May 27th, 2022"
output:
 rmarkdown::html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: haddock
    number_sections: true
---

# Raw 16s processing

Done on BU's SCC in linux (Terminal)

```{bash setup, eval=FALSE}
#*note: some of this was written by Dr. Carly D. Kenkel

#fastq files should have R1 & R2 designations for PE reads
#Also - some pre-trimming. Retain only PE reads that match amplicon primer. Remove reads containing Illumina sequencing adapters

#in Terminal home directory:
#following instructions of installing BBtools from https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/installation-guide/
#1. download BBMap package, sftp to installation directory
#2. untar: 
#tar -xvzf BBMap_(version).tar.gz
#3. test package:
#cd bbmap
#~/bin/bbmap/stats.sh in=~/bin/bbmap/resources/phix174_ill.ref.fa.gz

#primers for 16S: 
# >forward
# GTGYCAGCMGCCGCGGTA
# >reverse
# GGACTACHVGGGTWTCTAAT

##Still in terminal - making a sample list based on the first phrase before the underscore in the .fastq name
ls *R1_001.fastq | cut -d '_' -f 1 > samples.list

##cuts off the extra words in the .fastq files
for file in $(cat samples.list); do  mv ${file}_*R1*.fastq ${file}_R1.fastq; mv ${file}_*R2*.fastq ${file}_R2.fastq; done

##getting rid of first 4 bases (degenerate primers created them)
for file in $(cat samples.list); do ~/bin/bbmap/bbduk.sh in1=${file}_R1.fastq in2=${file}_R2.fastq ftl=4 out1=${file}_R1_No4N.fastq out2=${file}_R2_No4N.fastq; done &>bbduk_No4N.log

##only keeping reads that start with the 16S primer
for file in $(cat samples.list); do ~/bin/bbmap/bbduk.sh in1=${file}_R1_No4N.fastq in2=${file}_R2_No4N.fastq restrictleft=20 k=15 literal=GTGYCAGCMGCCGCGGTA,GGACTACHVGGGTWTCTAAT copyundefined=t outm1=${file}_R1_No4N_16S.fastq outu1=${file}_R1_check.fastq outm2=${file}_R2_No4N_16S.fastq outu2=${file}_R2_check.fastq; done &>bbduk_16S.log
##higher k = more reads removed, but can't surpass k=20 or 21

##using cutadapt to remove primer
module load python3/3.8.10
module load cutadapt/3.4

for file in $(cat samples.list)
do
cutadapt -g GTGYCAGCMGCCGCGGTA -a ATTAGAWACCCVHGTAGTCC -G GGACTACHVGGGTWTCTAAT -A TACCGCGGCKGCTGRCAC -n 2 --discard-untrimmed -o ${file}_R1_clipped.fastq -p ${file}_R2_clipped.fastq ${file}_R1_No4N_16S.fastq ${file}_R2_No4N_16S.fastq
done &> clip.log
##-g regular 5' forward primer
##-G regular 5' reverse primer
##-o forward out
##-p reverse out
##-max-n 0 means 0 Ns allowed

# did sftp of *_R1.fastq & *_R2.fastq files to the folder to be used in dada2
```

# Call ASVs with DADA2

Excellent walkthrough (v1.16) is [here](https://benjjneb.github.io/dada2/tutorial.html)

## Packages

```{r dada2 start, eval=FALSE}
#installing/loading packages:
#if (!requireNamespace("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")
#BiocManager::install("dada2") 
library(dada2); packageVersion("dada2")
#Version 1.16.0
library(ShortRead)
#packageVersion("ShortRead")
#1.46.0
library(Biostrings)
#packageVersion("Biostrings")
#2.56.0
path <- "~/nicfall@bu.edu - Google Drive/My Drive/Flirma/fl_16s/raw_temp2" # CHANGE ME to the directory containing the fastq files after unzipping.
```

## Read in data, check for primers

```{r dada2 werk, eval=FALSE}
fnFs <- sort(list.files(path, pattern = "_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq.gz", full.names = TRUE))

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][2]
sample.names <- unname(sapply(fnFs, get.sample.name))
head(sample.names)
sample.names

#### check for primers ####
FWD <- "GTGYCAGCMGCCGCGGTA"  ## CHANGE ME to your forward primer sequence
REV <- "GGACTACHVGGGTWTCTAAT"  ## CHANGE ME...

allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

#towards the beginning
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

#at the end
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[4]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[4]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[4]]))
```

No primers, amazing!

## Raw visuals{.tabset}

### Forward

```{r raw visuals, eval=FALSE}
#First, lets look at quality profile of R1 reads
plotQualityProfile(fnFs.filtN[c(1,2,3,4)])
plotQualityProfile(fnFs.filtN[c(88,89,90,91)])
plotQualityProfile(fnFs.filtN[c(263,264,265,266)])
#looks great up to 200, maybe 190 to be safe
```

### Reverse

```{r raw visuals rev, eval=FALSE}
#Then look at quality profile of R2 reads
plotQualityProfile(fnRs.filtN[c(1,2,3,4)])
plotQualityProfile(fnRs.filtN[c(88,89,90,91)])
plotQualityProfile(fnRs.filtN[c(263,264,265,266)])
#190ish again
```

Doing the rest on the cluster because I have too many samples

## Make ASV table

R script dada.R

```{bash, eval=FALSE}
nano dada.R
```

Text within

```{r dada.R script, eval=FALSE}
#now in "R env" on cluster
library("BiocManager")
#BiocManager::install("dada2") 
library(dada2)
packageVersion("dada2") #version 1.20
library(ShortRead)
packageVersion("ShortRead") #1.50.0
library(Biostrings)
packageVersion("Biostrings") #2.60.2

path <- "/projectnb2/davieslab/nicpro/flirma/fl16s" # CHANGE ME to the directory containing the fastq files

fnFs <- sort(list.files(path, pattern = "_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq.gz", full.names = TRUE))

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][2]
sample.names <- unname(sapply(fnFs, get.sample.name))
head(sample.names)
sample.names

# Make directory and filenames for the filtered fastqs
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

#changing a bit from default settings - maxEE=1 (1 max expected error, more conservative), truncating length at 175 bp for both forward & reverse [leaves ~50bp overlap], added "trimleft" to cut off primers [18 for forward, 20 for reverse]
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(190,190), #leaves ~50bp overlap
                     maxN=0, #DADA does not allow Ns
                     maxEE=c(1,1), #allow 1 expected errors, where EE = sum(10^(-Q/10)); more conservative, model converges
                     truncQ=2, 
                     #trimLeft=c(18,20), #N nucleotides to remove from the start of each read
                     rm.phix=TRUE, #remove reads matching phiX genome
                     matchIDs=TRUE, #enforce matching between id-line sequence identifiers of F and R reads
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

head(out)
tail(out)

#setDadaOpt(MAX_CONSIST=30) #increase number of cycles to allow convergence
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

#sanity check: visualize estimated error rates
#error rates should decline with increasing qual score
#red line is based on definition of quality score alone
#black line is estimated error rate after convergence
#dots are observed error rate for each quality score

#plotErrors(errF, nominalQ=TRUE) 
#plotErrors(errR, nominalQ=TRUE) 

### Dereplicate reads

#Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. 
#Dereplication substantially reduces computation time by eliminating redundant comparisons.
#DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent denoising step, significantly increasing DADA2’s accuracy.
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

#now, look at the dada class objects by sample
#will tell how many 'real' variants in unique input seqs
#By default, the dada function processes each sample independently, but pooled processing is available with pool=TRUE and that may give better results for low sampling depths at the cost of increased computation time. See our discussion about pooling samples for sample inference. 
dadaFs[[1]]
dadaRs[[1]]

### Merge paired reads

#To further cull spurious sequence variants
#Merge the denoised forward and reverse reads
#Paired reads that do not exactly overlap are removed

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

summary((mergers[[1]]))

#We now have a data.frame for each sample with the merged $sequence, its $abundance, and the indices of the merged $forward and $reverse denoised sequences. Paired reads that did not exactly overlap were removed by mergePairs.

seqtab <- makeSequenceTable(mergers)

dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

saveRDS(seqtab,file="fl16s.seqtab.rds")

plot(table(nchar(getSequences(seqtab)))) #real variants appear to be right in that 244-264 window
### note: I've done this part previously to look at the peak before proceeding with desired length trimming below

#The sequence table is a matrix with rows corresponding to (and named by) the samples, and 
#columns corresponding to (and named by) the sequence variants. 
#Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing

seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(244,264)] #again, being fairly conservative wrt length

#The core dada method removes substitution and indel errors, but chimeras remain. 
#Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
#than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
#a bimera (two-parent chimera) from more abundant sequences.

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
#Identified 1930 bimeras out of 77085 input sequences.

sum(seqtab.nochim)/sum(seqtab2)
#
#The fraction of chimeras varies based on factors including experimental procedures and sample complexity, 
#but can be substantial. 

# Track Read Stats #
 
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab2), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)

write.csv(track,file="fl16s.readstats.csv",row.names=TRUE,quote=FALSE)
saveRDS(seqtab.nochim,file="fl16s.seqtab.nochim.RDS")
```

Exit & save. Submit R script

```{bash, eval=FALSE}
nano dada.sh
```

Text within dada.sh

```
#!/bin/bash -l
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N dada.sh # job name, anything you want
#$ -l h_rt=24:00:00
#$ -M thenicolakriefall@gmail.com
#$ -m be

module load R
module load rstudio
Rscript dada.R
```

Exit & qsub

```{bash, eval=FALSE}
qsub -pe omp 16 dada.sh
```

# Assign taxonomy

Downloaded silva data [here; SILVA SSU r138 (modified) (151.8 MB)](http://www2.decipher.codes/Downloads.html)

For some reason I always have to re-install dada2 on the cluster:

```{bash, eval=FALSE}
BiocManager::install("dada2")
```

In cluster

```{bash, eval=FALSE}
nano taxid.R 
```

Text for script:
  
```{r taxid.R, eval=FALSE}
setwd("/projectnb/davieslab/nicpro/flirma/fl.2023/newtax")
seqtab.nochim <- readRDS("fl16s.seqtab.nochim.RDS")

#BiocManager::install("DECIPHER")
library(DECIPHER)
library("dada2")

sq.ids <- paste0("sq", seq(1, length(colnames(seqtab.nochim))))
#making output fasta file - did once then skipping
#path <- "/projectnb2/davieslab/nicpro/flirma/fl.2023/fl16s.23.fasta"
#uniquesToFasta(seqtab.nochim, path, ids = sq.ids, mode = "w", width = 20000)

dna <- DNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs
load("SILVA_SSU_r138_2019.RData") # CHANGE TO THE PATH OF YOUR TRAINING SET
ids <- IdTaxa(dna, trainingSet, strand="both", processors=NULL, verbose=T) # use all processors
ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species") # ranks of interest
# Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
taxid <- t(sapply(ids, function(x) {
  m <- match(ranks, x$rank)
  taxa <- x$taxon[m]
  taxa[startsWith(taxa, "unclassified_")] <- NA
  taxa
}))
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)
saveRDS(taxid,file="fl16s.23.taxid.rds")
#taxid <- readRDS("fl16s.23.taxid.rds")

colnames(seqtab.nochim)<-sq.ids
taxid2 <- cbind(taxid, rownames(taxid)) #retaining raw sequence info before renami$
rownames(taxid2)<-sq.ids

saveRDS(taxid2,file="fl16s.23.taxid.newids.rds")
saveRDS(seqtab.nochim,file="fl16s.23.seqtab.nochim.newids.rds")

sessionInfo()
```

Save & exit, make job:

```{bash, eval=FALSE}
nano taxid.23.sh
```

Text within:

```
#!/bin/bash -l
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N taxid.23.sh # job name, anything you want
#$ -l h_rt=48:00:00
#$ -M thenicolakriefall@gmail.com
#$ -m be

module load R
Rscript taxid.R
```

Submit job

```{bash, eval=FALSE}
qsub -pe omp 32 taxid.23.sh
```

```{r session info}
sessionInfo()
```

# Remove contaminants

```{r packages data etc., eval=FALSE}
library('phyloseq')
library('ggplot2')
#install.packages("Rmisc")
library('Rmisc')
library(cowplot)
#BiocManager::install("ShortRead")
library(ShortRead)

#### Read in previously saved datafiles ####
setwd("/Volumes/GoogleDrive/My Drive/Flirma/flirma_networks/fl_16s/01.generate_asv_table")
```

## Remove chloroplasts etc. 

```{r, eval=FALSE}
samdf <- read.csv("fl_sample_info - 16s_all_newvars.csv")
row.names(samdf) <- samdf$sample_num

#taxa <- readRDS("fl16s.rd3.taxa.newids.rds")
taxa <- readRDS("fl16s.23.taxid.newids.rds")

#seq.trim.newids <- readRDS("fl16s.rd3.seqtab.nochim.newids.rds")
#ncol(seq.trim.newids) 
##12,836 asvs

seq.newids <- readRDS("fl16s.23.seqtab.nochim.newids.rds")

#phyloseq object with new taxa ids
ps <- phyloseq(otu_table(seq.newids, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))

ps #12836
#75430, 360 samples untrimmed

# #### first look at data ####
# ps_glom <- tax_glom(ps.clean, "Family")
# plot_bar(ps_glom, x="site", fill="Family")+
#   theme(legend.position="none")

#### remove mitochondria, chloroplasts, non-bacteria #### 
ps.mito <- subset_taxa(ps, (family=="Mitochondria"))
ps.mito #17 taxa to remove
ps.chlor <- subset_taxa(ps, (order=="Chloroplast"))
ps.chlor #541 taxa to remove
ps.notbact <- subset_taxa(ps, (domain!="Bacteria") | is.na(domain))
ps.notbact #23591 taxa to remove

ps.nomito <- subset_taxa(ps, (family!="Mitochondria") | is.na(family))
ps.nomito #75413 taxa
ps.nochlor <- subset_taxa(ps.nomito, (order!="Chloroplast") | is.na(order))
ps.nochlor #74872 taxa
ps.clean <- subset_taxa(ps.nochlor, (domain=="Bacteria"))
ps.clean #51281 taxa

#just archaea
ps.arch <- subset_taxa(ps.nomito, (domain=="Archaea"))
ps.arch #485 taxa
```

## Remove contamination from negative controls

```{r decontam work, eval=FALSE}
#BiocManager::install("decontam")
library(decontam)

df <- as.data.frame(sample_data(ps.clean)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(ps.clean)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=year)) + geom_point()

sample_data(ps.clean)$lib_size <- sample_sums(ps.clean)
sample_data(ps.clean)$is.neg <- sample_data(ps.clean)$full_site == "neg"
contamdf.prev <- isContaminant(ps.clean, neg="is.neg",threshold=0.5)
table(contamdf.prev$contaminant)
# FALSE  TRUE - without trimming
# 51181   100 

# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa <- transform_sample_counts(ps.clean, function(abund) 1*(abund>0))
ps.pa.neg <- prune_samples(sample_data(ps.pa)$full_site == "neg", ps.pa)
ps.pa.pos <- prune_samples(sample_data(ps.pa)$full_site != "neg", ps.pa)
# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ps.pa.pos), pa.neg=taxa_sums(ps.pa.neg),
                    contaminant=contamdf.prev$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")

#remove from ps.clean:
ps.clean1 <- prune_taxa(!contamdf.prev$contaminant,ps.clean)
#also remove negative controls, don't need them anymore I think
ps.cleaner <- subset_samples(ps.clean1,(full_site!="neg"))

ps.cleaner 
#otu_table()   OTU Table:         [ 51181 taxa and 356 samples ]
#saveRDS(ps.cleaner,file="fl16s.ps.cleaner.rd3_newvars.rds")
#saveRDS(ps.cleaner,file="fl16s.23.ps.cleaner.rds")
```

# Rarefy

```{r rarefy raw - do once then save, eval=FALSE}
seqtab.cleaner <- data.frame(ps.cleaner@otu_table)

rarecurve(seqtab.cleaner,step=500,label=FALSE) #after removing contaminants

total <- rowSums(seqtab.cleaner)
fewer <- subset(total, total <5500)
#18 samples to remove, that's ~95% of samples which seems good
fewer

row.names.remove <- c("1192","1196","1200","1206","1207","1238","1143","1145","1146","1149","1171","1176","1071","1086","1088","827","850","1119")
#row.names.remove <- c("1196","1200","1206","1207","1236","1238","1143","1145","1146","1149","1171","1176","1071","1086","1088","827","850","1119")
seqtab.less <- seqtab.cleaner[!(row.names(seqtab.cleaner) %in% row.names.remove),]
#298 left

seqtab.rare <- rrarefy(seqtab.less,sample=5500)
rarecurve(seqtab.rare,step=500,label=FALSE)

#phyloseq object but rarefied
ps.rare <- phyloseq(otu_table(seqtab.rare, taxa_are_rows=FALSE), 
                    sample_data(samdf), 
                    tax_table(taxa))
ps.rare 
#[ 51181 taxa and 298 samples ]

#removing missing taxa - lost after rarefying
ps.rare <- prune_taxa(taxa_sums(ps.rare) > 0, ps.rare)
ps.rare #11897
#[ 39334 taxa and 298 samples ]

#saveRDS(ps.rare,file="fl16s.23.ps.cleaner.rare5500.rds")
ps.rare <- readRDS("fl16s.23.ps.cleaner.rare5500.rds")

#remove samples from cleaner that were removed in rarefaction
ps.less <- phyloseq(otu_table(seqtab.less, taxa_are_rows=FALSE), 
                    sample_data(samdf), 
                    tax_table(taxa))
ps.less
#otu_table()   OTU Table:         [ 51181 taxa and 298 samples ]

#removing missing taxa - lost after removing samples
ps.less <- prune_taxa(taxa_sums(ps.less) > 0, ps.less)
ps.less #11907
#otu_table()   OTU Table:         [ 48606 taxa and 298 samples ]

#saveRDS(ps.less,file="fl16s.23.ps.cleaner.less.rds")
```

Checking read depths after all this processing

```{r, eval=F}
ps.less <- readRDS("fl16s.23.ps.cleaner.less.rds")
sum(sample_sums(ps.less))
mean(sample_sums(ps.less))
#39593.51
sd(sample_sums(ps.less))
#27896.41

#saving some other info
samdf.cleaner <- data.frame(ps.cleaner@sam_data)
row.names(seqtab.cleaner)==row.names(samdf.cleaner)
samdf.cleaner$depth <- rowSums(seqtab.cleaner)
#write.csv(samdf.cleaner,file="samdf.cleaner.csv")
```







