---
title: "Florida 16S analysis"
author: "Nicola G. Kriefall"
date: "May 27th, 2022"
output:
 rmarkdown::html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: haddock
    number_sections: true
---

# Raw 16s processing

Done on BU's SCC in linux (Terminal)

```{bash setup, eval=FALSE}
#*note: some of this was written by Dr. Carly D. Kenkel

#fastq files should have R1 & R2 designations for PE reads
#Also - some pre-trimming. Retain only PE reads that match amplicon primer. Remove reads containing Illumina sequencing adapters

#in Terminal home directory:
#following instructions of installing BBtools from https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/installation-guide/
#1. download BBMap package, sftp to installation directory
#2. untar: 
#tar -xvzf BBMap_(version).tar.gz
#3. test package:
#cd bbmap
#~/bin/bbmap/stats.sh in=~/bin/bbmap/resources/phix174_ill.ref.fa.gz

#primers for 16S: 
# >forward
# GTGYCAGCMGCCGCGGTA
# >reverse
# GGACTACHVGGGTWTCTAAT

##Still in terminal - making a sample list based on the first phrase before the underscore in the .fastq name
ls *R1_001.fastq | cut -d '_' -f 1 > samples.list

##cuts off the extra words in the .fastq files
for file in $(cat samples.list); do  mv ${file}_*R1*.fastq ${file}_R1.fastq; mv ${file}_*R2*.fastq ${file}_R2.fastq; done

##getting rid of first 4 bases (degenerate primers created them)
for file in $(cat samples.list); do ~/bin/bbmap/bbduk.sh in1=${file}_R1.fastq in2=${file}_R2.fastq ftl=4 out1=${file}_R1_No4N.fastq out2=${file}_R2_No4N.fastq; done &>bbduk_No4N.log

##only keeping reads that start with the 16S primer
for file in $(cat samples.list); do ~/bin/bbmap/bbduk.sh in1=${file}_R1_No4N.fastq in2=${file}_R2_No4N.fastq restrictleft=20 k=15 literal=GTGYCAGCMGCCGCGGTA,GGACTACHVGGGTWTCTAAT copyundefined=t outm1=${file}_R1_No4N_16S.fastq outu1=${file}_R1_check.fastq outm2=${file}_R2_No4N_16S.fastq outu2=${file}_R2_check.fastq; done &>bbduk_16S.log
##higher k = more reads removed, but can't surpass k=20 or 21

##using cutadapt to remove primer
module load python3/3.8.10
module load cutadapt/3.4

for file in $(cat samples.list)
do
cutadapt -g GTGYCAGCMGCCGCGGTA -a ATTAGAWACCCVHGTAGTCC -G GGACTACHVGGGTWTCTAAT -A TACCGCGGCKGCTGRCAC -n 2 --discard-untrimmed -o ${file}_R1_clipped.fastq -p ${file}_R2_clipped.fastq ${file}_R1_No4N_16S.fastq ${file}_R2_No4N_16S.fastq
done &> clip.log
##-g regular 5' forward primer
##-G regular 5' reverse primer
##-o forward out
##-p reverse out
##-max-n 0 means 0 Ns allowed

# did sftp of *_R1.fastq & *_R2.fastq files to the folder to be used in dada2
```

# Call ASVs with DADA2

Excellent walkthrough (v1.16) is [here](https://benjjneb.github.io/dada2/tutorial.html)

## Packages

```{r dada2 start, eval=FALSE}
#installing/loading packages:
#if (!requireNamespace("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")
#BiocManager::install("dada2") 
library(dada2); packageVersion("dada2")
#Version 1.16.0
library(ShortRead)
#packageVersion("ShortRead")
#1.46.0
library(Biostrings)
#packageVersion("Biostrings")
#2.56.0
path <- "~/nicfall@bu.edu - Google Drive/My Drive/Flirma/fl_16s/raw_temp2" # CHANGE ME to the directory containing the fastq files after unzipping.
```

## Read in data, check for primers

```{r dada2 werk, eval=FALSE}
fnFs <- sort(list.files(path, pattern = "_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq.gz", full.names = TRUE))

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][2]
sample.names <- unname(sapply(fnFs, get.sample.name))
head(sample.names)
sample.names

#### check for primers ####
FWD <- "GTGYCAGCMGCCGCGGTA"  ## CHANGE ME to your forward primer sequence
REV <- "GGACTACHVGGGTWTCTAAT"  ## CHANGE ME...

allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

#towards the beginning
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

#at the end
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[4]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[4]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[4]]))
```

No primers, amazing!

## Raw visuals{.tabset}

### Forward

```{r raw visuals, eval=FALSE}
#First, lets look at quality profile of R1 reads
plotQualityProfile(fnFs.filtN[c(1,2,3,4)])
plotQualityProfile(fnFs.filtN[c(88,89,90,91)])
plotQualityProfile(fnFs.filtN[c(263,264,265,266)])
#looks great up to 200, maybe 190 to be safe
```

### Reverse

```{r raw visuals rev, eval=FALSE}
#Then look at quality profile of R2 reads
plotQualityProfile(fnRs.filtN[c(1,2,3,4)])
plotQualityProfile(fnRs.filtN[c(88,89,90,91)])
plotQualityProfile(fnRs.filtN[c(263,264,265,266)])
#190ish again
```

Doing the rest on the cluster because I have too many samples

## Make ASV table

R script dada.R

```{bash, eval=FALSE}
nano dada.R
```

Text within

```{r dada.R script, eval=FALSE}
#now in "R env" on cluster
library("BiocManager")
#BiocManager::install("dada2") 
library(dada2)
packageVersion("dada2") #version 1.20
library(ShortRead)
packageVersion("ShortRead") #1.50.0
library(Biostrings)
packageVersion("Biostrings") #2.60.2

path <- "/projectnb2/davieslab/nicpro/flirma/fl16s" # CHANGE ME to the directory containing the fastq files

fnFs <- sort(list.files(path, pattern = "_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq.gz", full.names = TRUE))

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][2]
sample.names <- unname(sapply(fnFs, get.sample.name))
head(sample.names)
sample.names

# Make directory and filenames for the filtered fastqs
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

#changing a bit from default settings - maxEE=1 (1 max expected error, more conservative), truncating length at 175 bp for both forward & reverse [leaves ~50bp overlap], added "trimleft" to cut off primers [18 for forward, 20 for reverse]
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(190,190), #leaves ~50bp overlap
                     maxN=0, #DADA does not allow Ns
                     maxEE=c(1,1), #allow 1 expected errors, where EE = sum(10^(-Q/10)); more conservative, model converges
                     truncQ=2, 
                     #trimLeft=c(18,20), #N nucleotides to remove from the start of each read
                     rm.phix=TRUE, #remove reads matching phiX genome
                     matchIDs=TRUE, #enforce matching between id-line sequence identifiers of F and R reads
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

head(out)
tail(out)

#setDadaOpt(MAX_CONSIST=30) #increase number of cycles to allow convergence
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

#sanity check: visualize estimated error rates
#error rates should decline with increasing qual score
#red line is based on definition of quality score alone
#black line is estimated error rate after convergence
#dots are observed error rate for each quality score

#plotErrors(errF, nominalQ=TRUE) 
#plotErrors(errR, nominalQ=TRUE) 

### Dereplicate reads

#Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. 
#Dereplication substantially reduces computation time by eliminating redundant comparisons.
#DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent denoising step, significantly increasing DADA2’s accuracy.
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

#now, look at the dada class objects by sample
#will tell how many 'real' variants in unique input seqs
#By default, the dada function processes each sample independently, but pooled processing is available with pool=TRUE and that may give better results for low sampling depths at the cost of increased computation time. See our discussion about pooling samples for sample inference. 
dadaFs[[1]]
dadaRs[[1]]

### Merge paired reads

#To further cull spurious sequence variants
#Merge the denoised forward and reverse reads
#Paired reads that do not exactly overlap are removed

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

summary((mergers[[1]]))

#We now have a data.frame for each sample with the merged $sequence, its $abundance, and the indices of the merged $forward and $reverse denoised sequences. Paired reads that did not exactly overlap were removed by mergePairs.

seqtab <- makeSequenceTable(mergers)

dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

saveRDS(seqtab,file="fl16s.seqtab.rds")

plot(table(nchar(getSequences(seqtab)))) #real variants appear to be right in that 244-264 window
### note: I've done this part previously to look at the peak before proceeding with desired length trimming below

#The sequence table is a matrix with rows corresponding to (and named by) the samples, and 
#columns corresponding to (and named by) the sequence variants. 
#Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing

seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(244,264)] #again, being fairly conservative wrt length

#The core dada method removes substitution and indel errors, but chimeras remain. 
#Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
#than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
#a bimera (two-parent chimera) from more abundant sequences.

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
#Identified 1930 bimeras out of 77085 input sequences.

sum(seqtab.nochim)/sum(seqtab2)
#
#The fraction of chimeras varies based on factors including experimental procedures and sample complexity, 
#but can be substantial. 

# Track Read Stats #
 
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab2), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)

write.csv(track,file="fl16s.readstats.csv",row.names=TRUE,quote=FALSE)
saveRDS(seqtab.nochim,file="fl16s.seqtab.nochim.RDS")
```

Exit & save. Submit R script

```{bash, eval=FALSE}
nano dada.sh
```

Text within dada.sh

```
#!/bin/bash -l
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N dada.sh # job name, anything you want
#$ -l h_rt=24:00:00
#$ -M thenicolakriefall@gmail.com
#$ -m be

module load R
module load rstudio
Rscript dada.R
```

Exit & qsub

```{bash, eval=FALSE}
qsub -pe omp 16 dada.sh
```

# Cleaning ASVs

## Trimming low abundance

Removing ASVs not well supported by sequencing data depth, in this case 0.0005% of total reads (<86 reads/ASV)

```{r, eval=FALSE}
seqtab.nochim <- readRDS("fl16s.seqtab.nochim.RDS")

ncol(seqtab.nochim)
#75,430 raw ASVs

sum(colSums(seqtab.nochim))
#16793981 (16 million reads)
#0.0005% = 84 reads

seq.raw.keep <- which(colSums(seqtab.nochim)*100/(sum(colSums(seqtab.nochim))) > 0.0005)
seq.trim <- seqtab.nochim[,seq.raw.keep]
ncol(seq.trim)
#12,836 ASVs remain

saveRDS(seq.trim,file="fl16s.seqtab.trim0005.rds")
```

## Assign taxonomy

### Dada2 method

In cluster

```{bash, eval=FALSE}
nano tax.R 
```

Text for script:
  
```{r tax.R, eval=FALSE}
library(dada2)

##trimmed:
seqtab.nochim <- readRDS("fl16s.seqtab.trim0005.rds")
##not trimmed:
#seqtab.nochim <- readRDS("fl16s_seqtab.nochim.rds")

ids <- paste0("sq", seq(1, length(colnames(seqtab.nochim))))
#making output fasta file - did once then skipping
path <- "/projectnb2/davieslab/nicpro/flirma/fl16s/flirma.rd3/fl16s.rd3.trim0005.fasta"
uniquesToFasta(seqtab.nochim, path, ids = ids, mode = "w", width = 20000)

taxa <- assignTaxonomy(seqtab.nochim, "silva_nr99_v138.1_wSpecies_train_set.fa.gz",tryRC=TRUE,verbose=TRUE)
unname(head(taxa))
saveRDS(taxa,file="fl16s.rd3.taxa.rds")

colnames(seqtab.nochim)<-ids
taxa2 <- cbind(taxa, rownames(taxa)) #retaining raw sequence info before renaming
rownames(taxa2)<-ids

saveRDS(taxa2,file="fl16s.rd3.taxa.newids.rds")
saveRDS(seqtab.nochim,file="fl16s.rd3.seqtab.nochim.newids.rds")
```

Save & exit, make job:

```{bash, eval=FALSE}
nano tax.rd3.sh
```

Text within:

```
#!/bin/bash -l
#$ -V # inherit the submission environment
#$ -cwd # start job in submission directory
#$ -N tax.rd3.sh # job name, anything you want
#$ -l h_rt=24:00:00
#$ -M thenicolakriefall@gmail.com
#$ -m be

module load R
Rscript tax.R
```

Submit job

```{bash, eval=FALSE}
qsub -pe omp 16 tax.rd3.sh
```

```{r session info}
sessionInfo()
```

## Remove contaminants

```{r packages data etc., eval=FALSE}
library('phyloseq')
library('ggplot2')
library('Rmisc')
library(cowplot)
library(ShortRead)

#### Read in previously saved datafiles ####
setwd("/Volumes/GoogleDrive/My Drive/Flirma/flirma_networks/fl_16s/01.generate_asv_table")
```

### Remove chloroplasts etc. 

```{r, eval=FALSE}
#samdf <- read.csv("fl_sample_info - 16s_all.csv")
samdf <- read.csv("fl_sample_info - 16s_all_newvars.csv")
row.names(samdf) <- samdf$sample_num

taxa <- readRDS("fl16s.rd3.taxa.newids.rds")

seq.trim.newids <- readRDS("fl16s.rd3.seqtab.nochim.newids.rds")
ncol(seq.trim.newids) 
#12,836 asvs

#phyloseq object with new taxa ids
ps <- phyloseq(otu_table(seq.trim.newids, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))

ps #12836

# #### first look at data ####
# ps_glom <- tax_glom(ps.clean, "Family")
# plot_bar(ps_glom, x="site", fill="Family")+
#   theme(legend.position="none")

#### remove mitochondria, chloroplasts, non-bacteria #### 
ps.mito <- subset_taxa(ps, (Family=="Mitochondria"))
ps.mito #131 taxa to remove
ps.chlor <- subset_taxa(ps, (Order=="Chloroplast"))
ps.chlor #334 taxa to remove
ps.notbact <- subset_taxa(ps, (Kingdom!="Bacteria") | is.na(Kingdom))
ps.notbact #186 taxa to remove

ps.nomito <- subset_taxa(ps, (Family!="Mitochondria") | is.na(Family))
ps.nomito #12705 taxa
ps.nochlor <- subset_taxa(ps.nomito, (Order!="Chloroplast") | is.na(Order))
ps.nochlor #12371 taxa
ps.clean <- subset_taxa(ps.nochlor, (Kingdom=="Bacteria"))
ps.clean #12185 taxa

#just archaea
ps.arch <- subset_taxa(ps.nomito, (Kingdom=="Archaea"))
ps.arch #186 taxa
```

### Remove contamination from negative controls

```{r decontam work, eval=FALSE}
#### identifying contamination ####
#BiocManager::install("decontam")
library(decontam)

df <- as.data.frame(sample_data(ps.clean)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(ps.clean)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=year)) + geom_point()

sample_data(ps.clean)$lib_size <- sample_sums(ps.clean)
sample_data(ps.clean)$is.neg <- sample_data(ps.clean)$full_site == "neg"
contamdf.prev <- isContaminant(ps.clean, neg="is.neg",threshold=0.5)
table(contamdf.prev$contaminant)
# FALSE  TRUE 
# 12108    77 
# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa <- transform_sample_counts(ps.clean, function(abund) 1*(abund>0))
ps.pa.neg <- prune_samples(sample_data(ps.pa)$full_site == "neg", ps.pa)
ps.pa.pos <- prune_samples(sample_data(ps.pa)$full_site != "neg", ps.pa)
# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ps.pa.pos), pa.neg=taxa_sums(ps.pa.neg),
                    contaminant=contamdf.prev$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")

#remove from ps.clean:
ps.clean1 <- prune_taxa(!contamdf.prev$contaminant,ps.clean)
#also remove negative controls, don't need them anymore I think
ps.cleaner <- subset_samples(ps.clean1,(full_site!="neg"))

ps.cleaner #12108 taxa left, 356 samples
#saveRDS(ps.cleaner,file="fl16s.ps.cleaner.rd3_newvars.rds")
```

### NCBI checks

Blast asvs to NCBI to see if any eukaryotes got through. Used 'fl16s.rd3.trim0005.fasta' made above. Split these up by 2500 ASVs so that it would be done overnight.... will need to repeat this 5 times since there are 12k+ ASVs.

In Terminal:

```{bash split fasta, eval=F}
#splitting by 2500 ASVs
awk -v size=2500 -v pre=fl16s_split -v pad=5 '
   /^>/ { n++; if (n % size == 1) { close(fname); fname = sprintf("%s.%0" pad "d", pre, n) } }
   { print >> fname }
' fl16s.rd3.trim0005.fasta

#necessary module
module load blast+

#create blast job
nano blast_taxid.sh
```

Text within job:

```
#!/bin/bash -l
#$ -l h_rt=24:00:00
#$ -cwd # start job in submission directory
#$ -N blast_taxid.sh # job name, anything you want
#$ -M thenicolakriefall@gmail.com
#$ -m be

blastn -query fl16s_split.00001 -db nt -outfmt "6 std staxids sskingdoms" -evalue 1e-5 -max_target_seqs 3 -out fl16s_taxids1.out -remote
```

```{bash, eval=F}
#exit & save
#submit:
qsub -pe omp 28 blast_taxid.sh
##takes a very long time

##now getting taxonomy info:

# #download/install taxonkit things, more instructions here:
# #https://bioinf.shenwei.me/taxonkit/usage/

# cd /net/scc-pa2/scratch/
# wget -c ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz 
# tar -zxvf taxdump.tar.gz
# cd 
# conda install -c bioconda taxonkit -p .
# cd /net/scc-pa2/scratch/taxa/
# cp *.dmp ~/.taxonkit
# #command taxonkit should work now

##extracting taxa ids from blast output for taxonkit:
#1-2500
awk -F " " '{print $13}' fl16s_taxids1.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_taxids1.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq > euk.contam.asvs1

#2501-5001
awk -F " " '{print $13}' fl16s_taxids2501.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_taxids2501.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq >> euk.contam.asvs1

#5001-7500
awk -F " " '{print $13}' fl16s_taxids5001.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_taxids5001.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq >> euk.contam.asvs1

#7501-10000
awk -F " " '{print $13}' fl16s_taxids7501.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_taxids7501.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq >> euk.contam.asvs1

#10001-12500
awk -F " " '{print $13}' fl16s_taxids10001.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_taxids10001.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq >> euk.contam.asvs1

#12501-12836
awk -F " " '{print $13}' fl16s_taxids12501.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_taxids12501.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq >> euk.contam.asvs1

#last few that didn't work the first time
awk -F " " '{print $13}' fl16s_lastfew.out > ids
taxonkit lineage ids > ids.tax
cut -f1 fl16s_lastfew.out > ids.seq; paste ids.seq ids.tax > ids.seq.tax
grep "Eukaryota" ids.seq.tax | cut -f1 | sort | uniq >> euk.contam.asvs1
```

Copied euk.contam.asvs1 into an excel/csv file, will remove from ps.cleaner. Should be *392* to remove.

```{r}
library("phyloseq")
setwd("/Volumes/GoogleDrive/My Drive/Flirma/flirma_networks/fl_16s/01.generate_asv_table")
ps.cleaner <- readRDS("fl16s.ps.cleaner.rd3_newvars.rds")
euks <- read.csv("fl16s_euk.contam.asvs.csv",header=FALSE)

euks_names <- euks$V1
alltaxa <- taxa_names(ps.cleaner) #should be 12108
keepers <- alltaxa[!(alltaxa %in% euks_names)] #only 77 got through from above
ps.cleanest <- prune_taxa(keepers, ps.cleaner) 
ps.cleanest
#12031 in ps.cleanest

seqtab.cleanest <- data.frame(otu_table(ps.cleanest))

##re-read in cleaned phyloseq object
#saveRDS(ps.cleanest,file="ps.cleanest.rds")
```

# Checking out the data

Read in data & packages

```{r read in data}
library(ggplot2)
library(vegan)
library(phyloseq)

setwd("/Volumes/GoogleDrive/My Drive/Flirma/flirma_networks/fl_16s/01.generate_asv_table")
#ps.temp <- readRDS("ps.cleanest.rds")
##exclude UK3
#ps.cleanest <- subset_samples(ps.temp,site!="UK3")
#saveRDS(ps.cleanest, "ps.cleanest.nouk3.rds")

ps.cleanest <- readRDS("ps.cleanest.nouk3.rds")
taxa <- readRDS("fl16s.rd3.taxa.newids.rds")
samdf <- read.csv("fl_sample_info - 16s_all_newvars.csv")
row.names(samdf) <- samdf$sample_num
```

## Raw & transformed data first{.tabset}

### Raw

Clear 2018 separation

```{r raw data}
ps.cleanest.ord <- ordinate(ps.cleanest, method="PCoA",distance="bray")
plot_ordination(ps.cleanest,ps.cleanest.ord,color="year")
plot_ordination(ps.cleanest,ps.cleanest.ord,color="lib_size")
```

### Relative abundance

Doesn't change much

```{r rel abun raw}
ps.cleanest.rel = transform_sample_counts(ps.cleanest, function(x) x / sum(x))

ps.cleanest.rel.ord <- ordinate(ps.cleanest.rel, method="PCoA",distance="bray")
plot_ordination(ps.cleanest.rel,ps.cleanest.rel.ord,color="year")
```

### Rarefy

```{r rarefy raw - do once then save, eval=FALSE}
seqtab.cleanest <- data.frame(ps.cleanest@otu_table)

rarecurve(seqtab.cleanest,step=500,label=FALSE) #after removing contaminants

total <- rowSums(seqtab.cleanest)
fewer <- subset(total, total <6000)
#18 samples to remove, that's ~95% of samples which seems good
fewer

row.names.remove <- c("1196","1200","1206","1207","1236","1238","1143","1145","1146","1149","1171","1176","1071","1086","1088","827","850","1119")
seqtab.less <- seqtab.cleanest[!(row.names(seqtab.cleanest) %in% row.names.remove),]
#298 left

seqtab.rare <- rrarefy(seqtab.less,sample=6000)
rarecurve(seqtab.rare,step=500,label=FALSE)

#phyloseq object but rarefied
ps.rare <- phyloseq(otu_table(seqtab.rare, taxa_are_rows=FALSE), 
                    sample_data(samdf), 
                    tax_table(taxa))
ps.rare

#removing missing taxa - lost after rarefying
ps.rare <- prune_taxa(taxa_sums(ps.rare) > 0, ps.rare)
ps.rare #11897

#saveRDS(ps.rare,file="ps.cleanest.rare6000.rds")

#remove samples from cleanest that were removed in rarefaction
ps.less <- phyloseq(otu_table(seqtab.less, taxa_are_rows=FALSE), 
                    sample_data(samdf), 
                    tax_table(taxa))
ps.less

#removing missing taxa - lost after removing samples
ps.less <- prune_taxa(taxa_sums(ps.less) > 0, ps.less)
ps.less #11907

#saveRDS(ps.less,file="ps.cleanest.less.rds")
```

#### Plot rarefied

Worse?

```{r rare}
ps.rare <- readRDS("ps.cleanest.rare6000.rds")
ps.rare.ord <- ordinate(ps.rare, method="PCoA",distance="bray")
plot_ordination(ps.rare,ps.rare.ord,color="year")
```

#### Rarefied, relative abundance

Doesn't change much

```{r rel abun rare}
ps.rare.rel = transform_sample_counts(ps.rare, function(x) x / sum(x))

ps.rare.rel.ord <- ordinate(ps.rare.rel, method="PCoA",distance="bray")
plot_ordination(ps.rare.rel,ps.rare.rel.ord,color="year")
```

Checking read depths after all this processing

```{r}
ps.less <- readRDS("ps.cleanest.less.rds")
mean(sample_sums(ps.less))
#40902.98
sd(sample_sums(ps.less))
#28402.7

#saving some other info
samdf.cleanest <- data.frame(ps.cleanest@sam_data)
row.names(seqtab.cleanest)==row.names(samdf.cleanest)
samdf.cleanest$depth <- rowSums(seqtab.cleanest)
#write.csv(samdf.cleanest,file="samdf.cleanest.csv")
```







